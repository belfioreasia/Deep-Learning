{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Coursework 2: Generative Models </h1>\n",
    "\n",
    "# Part 0 - Setup\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this coursework, you will build a generative latent diffusion model from scratch. Initially, you'll learn to create robust embeddings using a VAE. However, due to resource limitations, creating high-quality embeddings this way will prove too costly. Instead, we will use a pre-existing image encoder-decoder model from the backbone of Stable Diffusion 2. Based on this, you will train a latent diffusion model (Denoising Diffusion Probabilistic Models - DDPM with pretrained VAE latent embeddings). We will generate new images from pure noise, without prompt conditioning or other frills.\n",
    "\n",
    "Our goal is inspired by the famous hot-dog detector from the TV show Silicon Valley ([YouTube](https://www.youtube.com/watch?v=vIci3C4JkL0)), which turned out to be a more challenging machine learning problem than the show's creators might have anticipated. You will train a hot-dog generator using a clean and highly curated dataset. Afterwards, you'll test your model with a hot-dog detector we've provided to see if it can produce plausible hot-dog images.\n",
    "\n",
    "Please manage your expectations regarding image quality. Two examples from our training runs are shown below. We are working with smaller image sizes and have optimized this task for the available GPU resources and limited training time. If done correctly, you should be able to generate images that somewhat resemble hot dogs after only a few hours of training. For context, current generative models like Stable Diffusion require many days of training on large clusters, which is beyond the scope of this course. Nevertheless, this exercise will also introduce you to the use of large-scale GPU resources and their operation from a developer's perspective on a cluster.\n",
    "\n",
    "\n",
    "<img src=\"https://www.doc.ic.ac.uk/~bkainz/teaching/DL/best_example.png\" alt=\"Hot-dog Generator Model\" width=\"200\">\n",
    "<img src=\"https://www.doc.ic.ac.uk/~bkainz/teaching/DL/average_example.png\" alt=\"Hot-dog Generator Model\" width=\"200\">\n",
    "\n",
    "*Figure 1: One of the better Hot-dogs we managed to generate on small infrastructre (left) and an average result after 2h training (right).*\n",
    "\n",
    "\n",
    "## Submission \n",
    "Submissions will be done via LabTS (https://teaching.doc.ic.ac.uk/labts), similar to coursework 1. Please push your solution well in time and start early to avoid resource bottlenecks closer to the deadline. \n",
    "\n",
    "This notebook can be found in your gitlab LabTS generated gitlab repository. We also require you to clone our minimal base environment from https://github.com/bkainz/icl_dl_cw2_utils as automatically done when setting up the environment in code box 1. \n",
    "\n",
    "## Training\n",
    "Training the VAE and DDPM will take quite a long time (some hours), please refer to the DoC GPU options detailed in the logistics lecture (https://www.doc.ic.ac.uk/~bkainz/teaching/DL/L01_logistics_lecture.pdf).\n",
    "\n",
    "## Testing\n",
    "TAs will run a testing cell (at the end of this notebook), so you are required to copy your data ```transform``` and ```denorm``` functions to a cell near the bottom of the document (it is demarkated). You are advised to check that your implementations pass these tests (in particular, the jit saving and loading may not work for certain niche functions). \n",
    "\n",
    "The VAE and the DDPM are separate parts so that you can manage your time. Each can yield 50% of the marks but they do not depend on each other for successful completion.\n",
    "\n",
    "The DDPM will be tested with a hot-dog detector trained on https://universe.roboflow.com/workspace-2eqzv/hot-dog-detection/dataset/2 and https://www.kaggle.com/datasets/dansbecker/food-101. If one of your generated images is within the top-5 predicitons, save it and add to your repository. Your work is done at this point and if you implemented both parts correctly you will get 100% of the marks. \n",
    "\n",
    "## General\n",
    "Feel free to add architectural alterations / custom functions outside of pre-defined code blocks. However, if you manipulate the model's inputs in some way, please include the same code in the TA test cell, so our tests will run easily.\n",
    "\n",
    "The emphasis of both parts lies in understanding how the models behave and learn, however, some points will be available for getting good results with your LDM NOTE: you should not spend too long on this! Reasonable results are sufficent. It takes massive infrastructure and many GPU hours to train models like stable diffusion, so do NOT try to compete at this level here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up working environment\n",
    "You will need to install pytorch and import some utilities by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "# Dictionary mapping package names to their pip install names\n",
    "packages = {\n",
    "    'torch': 'torch torchvision altair seaborn',\n",
    "    'accelerate': 'accelerate', \n",
    "    'huggingface_hub': '--upgrade huggingface_hub',\n",
    "    'ipywidgets': 'ipywidgets',\n",
    "    'sklearn': 'scikit-learn',\n",
    "    'tqdm': 'tqdm',\n",
    "    'transformers': 'transformers',\n",
    "    'diffusers': 'diffusers[torch]',\n",
    "    'einops': 'einops'\n",
    "}\n",
    "\n",
    "# Check which packages are missing\n",
    "missing_packages = []\n",
    "for package, pip_name in packages.items():\n",
    "    if importlib.util.find_spec(package) is None:\n",
    "        missing_packages.append(pip_name)\n",
    "\n",
    "# Install all missing packages in a single pip call\n",
    "if len(missing_packages) > 0:\n",
    "    !pip install {' '.join(packages.values())}\n",
    "\n",
    "import os\n",
    "if not os.path.exists('icl_dl_cw2_utils'):\n",
    "    !git clone -q https://github.com/bkainz/icl_dl_cw2_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "\n",
    "from icl_dl_cw2_utils.utils.hotdogdataset import DLHotDogDataset\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image, make_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available CUDA devices and memory\n",
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    for i in range(num_devices):\n",
    "        device = torch.cuda.device(i)\n",
    "        total_mem = torch.cuda.get_device_properties(i).total_memory / 1024**3  # Convert to GB\n",
    "        allocated_mem = torch.cuda.memory_allocated(i) / 1024**3  # Convert to GB\n",
    "        free_mem = total_mem - allocated_mem\n",
    "        \n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Total Memory: {total_mem:.1f}GB\")\n",
    "        print(f\"Allocated Memory: {allocated_mem:.1f}GB\")\n",
    "        print(f\"Free Memory: {free_mem:.1f}GB\")\n",
    "        \n",
    "        if free_mem < 8:\n",
    "            print(f\"Warning: GPU {i} has less than 8GB of free VRAM!\")\n",
    "        else:\n",
    "            print(f\"Using GPU {i} with {free_mem:.1f}GB free VRAM\")\n",
    "            break \n",
    "    device = torch.device(f\"cuda:{i}\")\n",
    "else:\n",
    "    print(\"Warning: No CUDA devices available - running on CPU only\")\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have some default pathing options which vary depending on the environment you are using. You can of course change these as you please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization Cell\n",
    "WORKING_ENV = 'LABS' # Can be LABS, COLAB, PAPERSPACE, SAGEMAKER\n",
    "USERNAME = 'afs219' # If working on Lab Machines - Your college username\n",
    "assert WORKING_ENV in ['DOCCLUSTER', 'LABS', 'COLAB', 'LOCAL']\n",
    "                       \n",
    "if WORKING_ENV == 'COLAB':\n",
    "    from google.colab import drive\n",
    "    %load_ext google.colab.data_table\n",
    "    dl_cw2_repo_path = 'dl_cw2/' # path in your gdrive to the repo\n",
    "    content_path = f'/content/drive/MyDrive/{dl_cw2_repo_path}' # path to gitrepo in gdrive after mounting\n",
    "    data_path = './data/' # save the data locally\n",
    "    drive.mount('/content/drive/') # Outputs will be saved in your google drive\n",
    "\n",
    "elif WORKING_ENV == 'LABS' or WORKING_ENV == 'DOCCLUSTER':\n",
    "    content_path = f'/vol/bitbucket/{USERNAME}/dl/dl_cw2/' # You may want to change this\n",
    "    data_path = f'/vol/bitbucket/{USERNAME}/dl/data'\n",
    "    # Your python env and training data should be on bitbucket\n",
    "    if 'vol/bitbucket' not in content_path or 'vol/bitbucket' not in data_path:\n",
    "        import warnings\n",
    "        warnings.warn(\n",
    "           'It is best to create a dir in /vol/bitbucket/ otherwise you will quickly run into memory issues')\n",
    "               \n",
    "elif WORKING_ENV == 'LOCAL':\n",
    "    content_path = f'./dl/dl_cw2/' # You may want to change this\n",
    "    data_path = f'./dl/data'\n",
    "    os.makedirs(content_path, exist_ok=True)\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    # Your python env and training data should be on bitbucket\n",
    "    if './dl/dl_cw2/' not in content_path or './dl/data' not in data_path:\n",
    "        import warnings\n",
    "        warnings.warn(\n",
    "           'You know best what to do on your local machine')\n",
    "\n",
    "else:\n",
    "  raise NotImplementedError()\n",
    "\n",
    "content_path = Path(content_path)\n",
    "os.makedirs(content_path/'CW_VAE/', exist_ok=True)\n",
    "os.makedirs(content_path/'CW_LDM/', exist_ok=True)\n",
    "print(content_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "\n",
    "# We set a random seed to ensure that your results are reproducible.\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\"  if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "GPU = True # Choose whether to use GPU\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda\"  if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('you are using only the CPU. Please make sure that you train models on a GPU.')\n",
    "print(f'Using {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Variational Autoencoder (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Part 1.1 - Implementation and training (20 points)\n",
    " \n",
    " **Background:**\n",
    " \n",
    " Variational Autoencoders (VAEs) are generative models that learn to encode data into a lower-dimensional latent space and decode it back to reconstruct the original data. Unlike regular autoencoders, VAEs learn a probabilistic mapping by encoding inputs as distributions (typically Gaussian) in the latent space.\n",
    "\n",
    " The VAE architecture consists of:\n",
    " - An encoder network that maps input x to parameters (μ, σ) of the latent distribution q(z|x)\n",
    " - A sampling operation that draws latent vectors z from this distribution \n",
    " - A decoder network that reconstructs the input from z\n",
    "\n",
    " The loss function has two components:\n",
    " - Reconstruction loss: How well the decoder reconstructs the input\n",
    " - KL divergence loss: Ensures the latent distribution matches a prior p(z)\n",
    "\n",
    " **Your Task:**\n",
    " \n",
    " a. Implement a VAE with:\n",
    " - A convolutional encoder network that outputs μ and σ for the latent distribution\n",
    " - A sampling layer using the reparameterization trick\n",
    " - A convolutional decoder network\n",
    " - Appropriate hyperparameters (latent dimension, layer sizes etc.)\n",
    "\n",
    " More marks will be awarded for using convolutional layers vs. fully connected layers, as CNNs are better suited for image data.\n",
    " \n",
    " b. Implement the VAE loss function combining:\n",
    " - Reconstruction loss (e.g. Binary Cross Entropy for MNIST)\n",
    " - KL divergence between q(z|x) and p(z)\n",
    " Then train the model and visualize results.\n",
    " \n",
    " You will first implement and train the VAE on the MNIST dataset of handwritten digits. After validating your implementation, you will extend it to work with a more complex dataset of hot dog images.\n",
    "\n",
    " The following sections will guide you through:\n",
    " - Setting up the architecture\n",
    " - Implementing the loss function  \n",
    " - Training the model\n",
    " - Analyzing the latent space\n",
    " - Generating new samples\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1a - Implement VAE (10 Points)\n",
    "\n",
    "#### Hyper-parameter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Necessary Hyperparameters \n",
    "num_epochs = 16\n",
    "learning_rate = 0.01 #0.03\n",
    "batch_size = 128\n",
    "latent_dim = 20 # Choose a value for the size of the latent space\n",
    "\n",
    "# Additional Hyperparameters \n",
    "beta = 5\n",
    "lr_gamma = 0.1\n",
    "lr_step_size = 7\n",
    "\n",
    "# Mean and std of the training data\n",
    "mean = 0.1307\n",
    "std = 0.3081\n",
    "\n",
    "# (Optionally) Modify transformations on input\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Don't normalise since we will use the Binary Cross-Entropy loss\n",
    "    # as the data is nearly binary (see histogram plot in \"Defining a Loss\" section)\n",
    "    #transforms.Normalize(mean, std),\n",
    "\n",
    "    # We could add some data augmentation here (eg random rotations)\n",
    "    # but to keep it simple we won't\n",
    "])\n",
    "\n",
    "# (Optionally) Modify the network's output for visualizing your images\n",
    "def denorm(x):\n",
    "    #transforms.Normalize(-mean/std, 1/std)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "train_dat = datasets.MNIST(\n",
    "    data_path, train=True, download=True, transform=transform\n",
    ")\n",
    "test_dat = datasets.MNIST(data_path, train=False, transform=transform)\n",
    "\n",
    "loader_train = DataLoader(train_dat, batch_size, shuffle=True)\n",
    "loader_test = DataLoader(test_dat, batch_size, shuffle=False)\n",
    "\n",
    "# Don't change \n",
    "sample_inputs, _ = next(iter(loader_test))\n",
    "fixed_input = sample_inputs[:32, :, :, :]\n",
    "save_image(fixed_input, content_path/'CW_VAE/image_original.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    " #### Model Definition\n",
    " \n",
    " <figure>\n",
    "   <img src=\"https://blog.bayeslabs.co/assets/img/vae-gaussian.png\" style=\"width:60%\">\n",
    "   <figcaption>\n",
    "     Fig.1 - VAE Diagram (with a Guassian prior), taken from <a href=\"https://blog.bayeslabs.co/2019/06/04/All-you-need-to-know-about-Vae.html\">1</a>.\n",
    "   </figcaption>\n",
    " </figure>\n",
    " \n",
    " A Variational Autoencoder (VAE) consists of several key components:\n",
    " * Encoder: Compresses input data into a lower dimensional latent space\n",
    " * Reparametrization: Samples from the latent distribution in a differentiable way\n",
    " * Decoder: Reconstructs the original input from the latent representation\n",
    "\n",
    " You will need to define:\n",
    " * The hyperparameters - Control model capacity and training behavior\n",
    " * The constructor - Initialize model architecture and layers\n",
    " * encode - Maps input to mean and log variance of latent distribution\n",
    " * reparametrize - Samples latent vectors using reparametrization trick\n",
    " * decode - Maps latent vectors back to input space\n",
    " * forward - Combines all components into full forward pass\n",
    " \n",
    " Requirements:\n",
    " * This model MUST NOT have more than 1M parameters\n",
    " \n",
    " Hints:\n",
    " - It is common practice to encode the log of the variance, rather than the variance\n",
    "   (This improves numerical stability during training)\n",
    " - You might try using BatchNorm\n",
    "   (This can help stabilize training and reduce internal covariate shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# *CODE FOR PART 1.1a IN THIS CELL*\n",
    "\n",
    "# Define any deep CNN/MLP network class here if needed\n",
    "...\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, channels, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        ...\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "    \n",
    "    # We provide with a helper function in case it is useful\n",
    "    def make_layer_encode(self, block, in_channel, out_channel, stride, num_blocks, expand_ratio):\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            if i == 1:\n",
    "                s = stride\n",
    "            else:\n",
    "                s = 1\n",
    "            layers.append(block(in_channel, out_channel, s, expand_ratio))\n",
    "            in_channel = out_channel\n",
    "        return nn.Sequential(*layers)    \n",
    "\n",
    "    def encode(self, x):\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        ...\n",
    "        return mu, logvar\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "    \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        ...\n",
    "        return z\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "\n",
    "    def decode(self, z):\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        ...\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        ...\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "        return out, mu, logvar\n",
    "\n",
    "model = VAE(1, latent_dim).to(device)\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters is: {}\".format(params))\n",
    "print(model)\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q1.1a Implement VAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1b - Training the Model (5 Points)\n",
    "\n",
    "#### Defining a Loss\n",
    "Recall the Beta VAE loss, with an encoder $q$ and decoder $p$:\n",
    "$$ \\mathcal{L}=\\mathbb{E}_{q_\\phi(z \\mid X)}[\\log p_\\theta(X \\mid z)]-\\beta D_{K L}[q_\\phi(z \\mid X) \\| p_\\theta(z)]$$\n",
    "\n",
    "In order to implement this loss you will need to think carefully about your model's outputs and the choice of prior.\n",
    "\n",
    "There are multiple accepted solutions. Explain your design choices based on the assumptions you make regarding the distribution of your data.\n",
    "\n",
    "* Hint: this refers to the log likelihood as mentioned in the tutorial. Make sure these assumptions reflect on the values of your input data, i.e. depending on your choice you might need to do a simple preprocessing step.\n",
    "\n",
    "* You are encouraged to experiment with the weighting coefficient $\\beta$ and observe how it affects your training\n",
    "\n",
    "* This will take between 5-20 min training time for 15 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# *CODE FOR PART 1.1b IN THIS CELL*\n",
    "\n",
    "def loss_function_VAE(recon_x, x, mu, logvar, beta):\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        ...\n",
    "        return NLL + KLD, NLL, KLD\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "\n",
    "model = VAE(1, latent_dim).to(device)\n",
    "model_path = f\"./VAE_model_{latent_dim}.pth\"\n",
    "\n",
    " #optional -- do this if you want to continue training on a previous training run\n",
    "load_checkpoint = False\n",
    "if os.path.exists(model_path) and load_checkpoint:\n",
    "    print(\"loading existing model...\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = lr_step_size, gamma = lr_gamma)\n",
    "\n",
    "num_epochs = 1 #! For testing\n",
    "\n",
    "for epoch in range(num_epochs):     \n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "       \n",
    "        total_loss, total_NLL_loss, total_KLD_loss = 0, 0, 0\n",
    "        model.train()\n",
    "        for data, y in loader_train:\n",
    "            ...\n",
    "        train_losses.append([total_loss / len(loader_train.dataset), total_NLL_loss / len(loader_train.dataset), total_KLD_loss / len(loader_train.dataset)])\n",
    "\n",
    "        total_loss, total_NLL_loss, total_KLD_loss = 0, 0, 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, y in loader_test:\n",
    "                ...\n",
    "        test_losses.append([total_loss / len(loader_test.dataset), total_NLL_loss / len(loader_test.dataset), total_KLD_loss / len(loader_test.dataset)])\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Epoch: {epoch}, train loss = {train_losses[-1][0]:.2f}, test loss = {test_losses[-1][0]:.2f}, lr = {lr:.5f}')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "        # save the model\n",
    "        if epoch % 5 == 0:\n",
    "            with torch.no_grad():\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                \n",
    "        if epoch == num_epochs - 1:\n",
    "            with torch.no_grad():\n",
    "                torch.jit.save(torch.jit.trace(model, (data), check_trace=False),\n",
    "                    'VAE_model_jit.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Part 1.1c - Data Exploration and choice of loss (5 Points)\n",
    "\n",
    "The data distribution is very important when deciding which loss to implement for the decoder $p(x|z)$. Compute the following:\n",
    "\n",
    "* Histogram of the input data domain.\n",
    "* Reconstruction loss assuming $p(x|z)$ is Gaussian distributed.\n",
    "* Reconstruction loss assuming $p(x|z)$ is Bernoulli distributed.\n",
    "\n",
    "Reflect on your implementation and make any necessary changes to your final model (choice loss function and decoder activation functions) by looking at the histogram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Any code for your explanation here\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#######################################################################\n",
    "#                       ** START OF YOUR CODE **\n",
    "#######################################################################\n",
    "        \n",
    "# Set-up\n",
    "x, _ = next(iter(loader_test))\n",
    "x = x.to(device)[0:32, :, :, :]\n",
    "\n",
    "...\n",
    "\n",
    "print('Gaussian distribution')\n",
    "print(f'Normal log prob:                 {loss1:.3f}')\n",
    "print(f'MSE:                             {loss2:.3f}')\n",
    "print(f'MSE + constant terms = log prob: {loss3:.3f}')\n",
    "print()\n",
    "print('Multinulli distribution')\n",
    "print(f'Cross entropy:              {loss4:.3f}')\n",
    "print('Bernoulli distribution')\n",
    "print(f'Bernoulli log prob:              {loss4:.3f}')\n",
    "print(f'BCE:                             {loss5:.3f}')\n",
    "\n",
    "all_train_dataloader = DataLoader(train_dat, len(train_dat))\n",
    "data = next(iter(all_train_dataloader))\n",
    "data = data[0].flatten().numpy()\n",
    "plt.hist(data, bins=100)\n",
    "plt.grid()\n",
    "plt.title('Histogram of the training data\\nshowing nearly binomial distribution')\n",
    "\n",
    "#######################################################################\n",
    "#                       ** END OF YOUR CODE **\n",
    "####################################################################### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Part 1.2 - Evaluation (10 points)\n",
    "\n",
    "a. Plot your loss curves\n",
    "\n",
    "b. Show reconstructions and samples\n",
    "\n",
    "### Part 1.2a - Loss Curves (3 Points)\n",
    "Plot your loss curves (6 in total, 3 for the training set and 3 for the test set): total loss, reconstruction log likelihood loss, KL loss (x-axis: epochs, y-axis: loss). If you experimented with different values of $\\beta$, you may wish to display multiple plots (worth 1 point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# *CODE FOR PART 1.2a IN THIS CELL*\n",
    "#######################################################################\n",
    "#                       ** START OF YOUR CODE **\n",
    "#######################################################################\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "train_losses_ = ...\n",
    "test_losses_ = ...\n",
    "\n",
    "train_l = ...\n",
    "test_l = ...\n",
    "labels = ['total', 'NLL', 'KL']\n",
    "...\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss curves for beta = 5, latent vectors = 20')\n",
    "#######################################################################\n",
    "#                       ** END OF YOUR CODE **\n",
    "####################################################################### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Part 1.2b - Samples and Reconstructions (7 Points)\n",
    "Visualize a subset of the images of the test set and their reconstructions **as well as** a few generated samples. Most of the code for this part is provided. You only need to call the forward pass of the model for the given inputs (might vary depending on your implementation). (4 points)\n",
    "\n",
    "For reference, here's [some samples from our VAE](https://imgur.com/NwNMuG3).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# *CODE FOR PART 1.2b IN THIS CELL*\n",
    "\n",
    "# load the model\n",
    "print('Input images')\n",
    "print('-'*50)\n",
    "\n",
    "sample_inputs, _ = next(iter(loader_test))\n",
    "fixed_input = sample_inputs[0:32, :, :, :]\n",
    "# visualize the original images of the last batch of the test set\n",
    "img = make_grid(denorm(fixed_input), nrow=8, padding=2, normalize=False,\n",
    "                scale_each=False, pad_value=0)\n",
    "plt.figure()\n",
    "show(img)\n",
    "\n",
    "print('Reconstructed images')\n",
    "print('-'*50)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # visualize the reconstructed images of the last batch of test set\n",
    "    \n",
    "    #######################################################################\n",
    "    #                       ** START OF YOUR CODE **\n",
    "    #######################################################################\n",
    "    ...\n",
    "    #######################################################################\n",
    "    #                       ** END OF YOUR CODE **\n",
    "    ####################################################################### \n",
    "    \n",
    "    recon_batch = recon_batch.cpu()\n",
    "    recon_batch = make_grid(denorm(recon_batch), nrow=8, padding=2, normalize=False,\n",
    "                             scale_each=False, pad_value=0)\n",
    "    plt.figure()\n",
    "    show(recon_batch)\n",
    "\n",
    "print('Generated Images')  \n",
    "print('-'*50)\n",
    "model.eval()\n",
    "n_samples = 256\n",
    "z = torch.randn(n_samples,latent_dim).to(device)\n",
    "with torch.no_grad():\n",
    "    #######################################################################\n",
    "    #                       ** START OF YOUR CODE **\n",
    "    #######################################################################\n",
    "    ...\n",
    "    #######################################################################\n",
    "    #                       ** END OF YOUR CODE **\n",
    "    ####################################################################### \n",
    "    \n",
    "    samples = samples.cpu()\n",
    "    samples = make_grid(denorm(samples), nrow=16, padding=2, normalize=False,\n",
    "                             scale_each=False, pad_value=0)\n",
    "    plt.figure(figsize = (8,8))\n",
    "    show(samples)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Discussion\n",
    "Provide a brief analysis of your loss curves and reconstructions: \n",
    "* What do you observe in the behaviour of the log-likelihood loss and the KL loss (increasing/decreasing)?\n",
    "* Can you intuitively explain if this behaviour is desirable? \n",
    "* What is posterior collapse and did you observe it during training (i.e. when the KL is too small during the early stages of training)? \n",
    "    * If yes, how did you mitigate it? How did this phenomenon reflect on your output samples?\n",
    "    * If no, why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Your Answer (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "## Part 1.3 - Latent Space Exploration (10 points)\n",
    "\n",
    "In this question you are asked to qualitatively assess the learned representations that your model has learned. In particular:\n",
    "\n",
    "a. Dimensionality Reduction of learned embeddings\n",
    "\n",
    "b. Interpolating in the latent space\n",
    "\n",
    "### Part 1.3a - T-SNE on Embeddings (5 Points)\n",
    "Extract the latent representations of the test set and visualize them using [T-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)  [(see implementation)](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). You can use a T-SNE implementation from a library such as scikit-learn. \n",
    "\n",
    "You are encouraged to also produce a matplotlib plot (please use different colours for each digit class) with different (max 4) perplexity values (e.g. 3, 30, 50)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# *CODE FOR PART 1.3a IN THIS CELL\n",
    "#######################################################################\n",
    "#                       ** START OF YOUR CODE **\n",
    "#######################################################################\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#! You can vary the size of the test set for quick debugging. We recommend 1000 to 10000 to final tests\n",
    "test_dataloader = DataLoader(test_dat, 100, shuffle=False)\n",
    "\n",
    "# Get latent dimensions for test set\n",
    "...\n",
    "# Perform TSNE\n",
    "print('Performing TSNE')\n",
    "z_embedded_5 = ...\n",
    "z_embedded_30 = ...\n",
    "z_embedded_50 = ...\n",
    "#######################################################################\n",
    "#                       ** END OF YOUR CODE **\n",
    "####################################################################### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(19.2, 4.8))\n",
    "labels = [5, 30, 50]\n",
    "z_list = [z_embedded_5, z_embedded_30, z_embedded_50]\n",
    "for i in range(len(labels)):\n",
    "    for digit in np.unique(y):\n",
    "        idx = np.where(y == digit)\n",
    "        ax[i].scatter(z_list[i][idx,0], z_list[i][idx,1], label=digit)\n",
    "    ax[i].legend()\n",
    "    ax[i].set_title(f'TSNE plot for beta = 5, latent vectors = 20, perplexity = {labels[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3b - Interpolating in $z$ (5 Points)\n",
    "\n",
    "Perform a linear interpolation in the latent space of the autoencoder by choosing any two digits from the test set. What do you observe regarding the transition from on digit to the other?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CODE FOR PART 1.3b IN THIS CELL\n",
    "#######################################################################\n",
    "#                       ** START OF YOUR CODE **\n",
    "#######################################################################\n",
    "num_steps = 10\n",
    "...\n",
    "image1 = ...\n",
    "image2 = ...\n",
    "...\n",
    "interpolated_embeddings = ...\n",
    "\n",
    "with torch.no_grad():\n",
    "    interpolated_images = ...\n",
    "\n",
    "def imshow(img, ax):\n",
    "    img = img.permute(1, 2, 0)\n",
    "    ax.imshow(img.cpu().numpy())\n",
    "    ax.axis('off')\n",
    "\n",
    "fig, axes = plt.subplots(1, interpolated_images.size(0) + 2, figsize=(20, 5))\n",
    "imshow(image1[0], axes[0])\n",
    "axes[0].set_title('Original Image 1')\n",
    "for i in range(interpolated_images.size(0)):\n",
    "    imshow(interpolated_images[i], axes[i + 1])\n",
    "axes[1].set_title('Interpolated Images')\n",
    "imshow(image2[0], axes[-1])\n",
    "axes[-1].set_title('Original Image 2')\n",
    "plt.show()\n",
    "#######################################################################\n",
    "#                       ** END OF YOUR CODE **\n",
    "####################################################################### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    " ## Part 1.4 - Hot Dog Generation (10 points)\n",
    "\n",
    " Now we'll apply our VAE to the more challenging task of generating hot dog images. This requires adapting our model to handle:\n",
    "\n",
    " - RGB images (3 channels instead of 1)\n",
    "\n",
    " - More complex visual features\n",
    "\n",
    " - Higher resolution inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Now let's adapt our VAE for hot dog image generation\n",
    "# The DLHotDogDataset class will automatically download and prepare the dataset\n",
    "# Since our VAE architecture is relatively simple, we'll start with the same \n",
    "# resolution as MNIST (28x28) to establish a baseline\n",
    "# Note this can take a while to load\n",
    "\n",
    "# Define image parameters\n",
    "image_size = 28  # Same size as MNIST for initial experiments\n",
    "batch_size = 64  # Process 64 images at a time\n",
    "\n",
    "# Define training data transforms\n",
    "# - ToTensor: Convert PIL images to tensors and scale to [0,1]\n",
    "# - Normalize: Rescale to [-1,1] using mean=(0.5,0.5,0.5) and std=(0.5,0.5,0.5)\n",
    "# - Resize: Scale images to 28x28\n",
    "# - Random flips: Data augmentation to help prevent overfitting\n",
    "transform_ = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "     transforms.Resize((image_size, image_size)),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.RandomVerticalFlip(p=0.5)]\n",
    "     )\n",
    "\n",
    "# Define test data transforms - same as training but without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((image_size, image_size))\n",
    "])\n",
    "\n",
    "# Create train and test datasets\n",
    "hotdogdata_train = DLHotDogDataset(root=data_path, transform=transform_, split='train')\n",
    "hotdogdata_test = DLHotDogDataset(root=data_path, transform=transform_test, split='test')\n",
    "\n",
    "# Create data loaders for batched processing\n",
    "# Shuffle training data but keep test data ordered\n",
    "hotdogdata_loader_train = DataLoader(hotdogdata_train, batch_size, shuffle=True)\n",
    "hotdogdata_loader_test = DataLoader(hotdogdata_test, batch_size, shuffle=False)\n",
    "\n",
    "# Save a batch of original test images for later comparison\n",
    "# Takes first 32 images from test set\n",
    "sample_inputs_, _ = next(iter(hotdogdata_loader_test))\n",
    "fixed_input_ = sample_inputs_[:32, :, :, :]\n",
    "save_image(fixed_input_, content_path/'CW_VAE/image_original_hd.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Part 1.4a - Train the VAE (4 points)\n",
    "\n",
    "\n",
    "\n",
    " * here you will need to assume a normal distribution for your loss function unlike above\n",
    "\n",
    " * the data also has 3 channels now instead of only one\n",
    "\n",
    " * otherwise the VAE can be trained in the same way as for MNIST above\n",
    "\n",
    " * training will take 60 min for this dataset for 30 epochs (more samples) but results will be limited as shown in Fig 3. (except if your VAE is brilliant ;) ).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "#We need a bit more training as for MNIST but don't expect much\n",
    "num_epochs = 60\n",
    "\n",
    "def loss_function_VAE(recon_x, x, mu, logvar, beta):\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################  \n",
    "        # Assume Normal distribution\n",
    "        ...\n",
    "        return NLL + KLD, NLL, KLD\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "\n",
    "model = VAE(3, latent_dim).to(device)\n",
    "model_path = f\"./VAE_hd_model_{latent_dim}.pth\"\n",
    "\n",
    " #optional -- do this if you want to continue training on a previous training run\n",
    "load_checkpoint = False\n",
    "if os.path.exists(model_path) and load_checkpoint:\n",
    "    print(\"loading existing model...\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = lr_step_size, gamma = lr_gamma)\n",
    "for epoch in range(num_epochs):     \n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        ...\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "        # save the model\n",
    "        if epoch % 5 == 0:\n",
    "            with torch.no_grad():\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                \n",
    "        if epoch == num_epochs - 1:\n",
    "            with torch.no_grad():\n",
    "                torch.jit.save(torch.jit.trace(model, (data), check_trace=False),\n",
    "                    'VAE_hd_model_jit.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    " #### Part 1.4b - Sample Visualisation (3 points)\n",
    "\n",
    "\n",
    "\n",
    " <img src=\"https://www.doc.ic.ac.uk/~bkainz/teaching/DL/bad_VAE.png\" alt=\"Hot-dog Generator Model\" width=\"100\">\n",
    "\n",
    "\n",
    "\n",
    " *Figure 3: do not expect samples getting much better than this.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# *CODE FOR PART 1.2b IN THIS CELL*\n",
    "\n",
    "# load the model\n",
    "print('Input images')\n",
    "print('-'*50)\n",
    "\n",
    "sample_inputs, _ = next(iter(hotdogdata_loader_test))\n",
    "fixed_input = sample_inputs[0:32, :, :, :]\n",
    "# visualize the original images of the last batch of the test set\n",
    "img = make_grid(denorm(fixed_input), nrow=8, padding=2, normalize=False,\n",
    "                range=None, scale_each=False, pad_value=0)\n",
    "plt.figure()\n",
    "show(img)\n",
    "\n",
    "print('Reconstructed images')\n",
    "print('-'*50)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # visualize the reconstructed images of the last batch of test set\n",
    "    \n",
    "    #######################################################################\n",
    "    #                       ** START OF YOUR CODE **\n",
    "    #######################################################################\n",
    "    ...\n",
    "    #######################################################################\n",
    "    #                       ** END OF YOUR CODE **\n",
    "    ####################################################################### \n",
    "    \n",
    "    recon_batch = recon_batch.cpu()\n",
    "    recon_batch = make_grid(denorm(recon_batch), nrow=8, padding=2, normalize=False,\n",
    "                            range=None, scale_each=False, pad_value=0)\n",
    "    plt.figure()\n",
    "    show(recon_batch)\n",
    "\n",
    "print('Generated Images')  \n",
    "print('-'*50)\n",
    "model.eval()\n",
    "n_samples = 256\n",
    "z = torch.randn(n_samples,latent_dim).to(device)\n",
    "with torch.no_grad():\n",
    "    #######################################################################\n",
    "    #                       ** START OF YOUR CODE **\n",
    "    #######################################################################\n",
    "    ...\n",
    "    #######################################################################\n",
    "    #                       ** END OF YOUR CODE **\n",
    "    ####################################################################### \n",
    "    \n",
    "    samples = samples.cpu()\n",
    "    samples = make_grid(denorm(samples), nrow=16, padding=2, normalize=False,\n",
    "                            range=None, scale_each=False, pad_value=0)\n",
    "    plt.figure(figsize = (8,8))\n",
    "    show(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    " #### Part 1.4c - Interpolating in $z$ (3 points)\n",
    "\n",
    "\n",
    "\n",
    " Perform a linear interpolation in the latent space of the autoencoder by choosing any two digits from the test set. What do you observe regarding the transition from on digit to the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# CODE FOR PART 1.3b IN THIS CELL\n",
    "#######################################################################\n",
    "#                       ** START OF YOUR CODE **\n",
    "#######################################################################\n",
    "num_steps = 10\n",
    "...\n",
    "image1 = ...\n",
    "image2 = ...\n",
    "...\n",
    "interpolated_embeddings = ...\n",
    "\n",
    "with torch.no_grad():\n",
    "    interpolated_images = ...\n",
    "\n",
    "\n",
    "def imshow(img, ax):\n",
    "    img = img.permute(1, 2, 0)\n",
    "    ax.imshow(img.cpu().numpy())\n",
    "    ax.axis('off')\n",
    "\n",
    "fig, axes = plt.subplots(1, interpolated_images.size(0) + 2, figsize=(20, 5))\n",
    "imshow(image1[0], axes[0])\n",
    "axes[0].set_title('Original Image 1')\n",
    "for i in range(interpolated_images.size(0)):\n",
    "    imshow(interpolated_images[i], axes[i + 1])\n",
    "axes[1].set_title('Interpolated Images')\n",
    "imshow(image2[0], axes[-1])\n",
    "axes[-1].set_title('Original Image 2')\n",
    "plt.show()\n",
    "#######################################################################\n",
    "#                       ** END OF YOUR CODE **\n",
    "####################################################################### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    " <h3> oh boy, this looks terribe... </h3>\n",
    "\n",
    "\n",
    "\n",
    " let's use a more powerful pretrained VAE in Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Denoising Diffusion Probabilistic Model (DDPM) (50 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the VAE above is undertrained to represent hot dogs well, we try to use one of the most powerful representation embeddings to date, the VAE of Stable Diffusion. This we can get from HuggingFace. If the provided token does not work, get a huggingface.com account and generate your own access token. This is required to download the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# We specified data path in part 1\n",
    "cache_path = os.path.join(data_path, \"cache\")\n",
    "print('Caching models in', cache_path)\n",
    "%env XDG_CACHE_HOME=cache_path\n",
    "%env HF_HOME=cache_path\n",
    "\n",
    "import torch \n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from diffusers import AutoencoderKL\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.optim import Adam\n",
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from typing import Any, Callable, Optional, Tuple, Union\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Loading a pretrained VAE from HuggingFace </h2>\n",
    " Let's do what many generative AI researchers do right now and connect to HuggingFace's transformer library and model zoo.\n",
    " This will give us access to powerful pretrained models like Stable Diffusion's VAE.\n",
    "\n",
    " To access the pretrained models from HuggingFace, we need a write access token.\n",
    " This token acts like a password that gives us permission to download models from HuggingFace's model hub.\n",
    " \n",
    " When you run the cell below, you'll be prompted to login to HuggingFace if you don't have a token.\n",
    " After logging in, make sure to select \"Write\" access when generating your token, as shown in the screenshot.\n",
    " The write access token will allow us to not only read but also potentially contribute models back to the community.\n",
    " \n",
    " <img src=\"https://www.doc.ic.ac.uk/~afs219/hf_token_example.png\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, HfFolder\n",
    "\n",
    "# Get token from environment variable or prompt user to login\n",
    "manual_token = \"<your-huggingface-token>\"\n",
    "if manual_token is None:\n",
    "    token = HfFolder.get_token()\n",
    "    if token is None:\n",
    "        print(\"Please login to Hugging Face to get a token\")\n",
    "        login(add_to_git_credential=True)\n",
    "else:\n",
    "    token = manual_token\n",
    "    login(token=token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following provides a simple wrapper for Stable Diffusion's extensively trained VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a wrapper for Stable Diffusion's VAEs \n",
    "class VAE: \n",
    "    def __init__(self, device=\"cuda\") -> None:\n",
    "        vae = AutoencoderKL.from_pretrained(\n",
    "            \"stabilityai/stable-diffusion-2-base\", subfolder=\"vae\",\n",
    "        )\n",
    "\n",
    "        vae = vae.to(device)\n",
    "        self.vae = vae\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # assumes 0-1 normalized image\n",
    "        single_image = False\n",
    "        if len(x.size()) == 3: \n",
    "            single_image = True\n",
    "            x = x.unsqueeze(dim=0)\n",
    "        \n",
    "        if x.min() >= 0: \n",
    "            x = x * 2 - 1 \n",
    "\n",
    "        with torch.no_grad():\n",
    "            encode = self.vae.encode(x.cuda())\n",
    "            batch = encode.latent_dist.sample() *  self.vae.config.scaling_factor\n",
    "        if single_image: \n",
    "            batch = batch.squeeze(dim=0)\n",
    "        return batch\n",
    "\n",
    "    def encode(self, x): \n",
    "        return self(x)\n",
    "    \n",
    "    def decode(self, z): \n",
    "        \"\"\"\n",
    "        returns: decode image\"\"\"\n",
    "        if len(z.size()) == 3: \n",
    "            z = z.unsqueeze(dim=0)\n",
    "        with torch.no_grad():\n",
    "            x = self.vae.decode(z / self.vae.config.scaling_factor, return_dict=False)[0]\n",
    "            #x = self.vae.decode(z , return_dict=False)[0]\n",
    "\n",
    "        x = ((1 + x) * 0.5).clip(0, 1)\n",
    "        return x\n",
    "\n",
    "sd_vae = VAE()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Part 2.1 - Exploring the Stable Diffusion VAE (3 points)\n",
    " \n",
    " Now that we have access to a pretrained VAE from Stable Diffusion, let's explore its capabilities and compare its performance to our trained VAE from Part 1. We'll perform similar experiments to analyze the quality and characteristics of its latent space:\n",
    "\n",
    " 1. Linear interpolation between embeddings to examine the smoothness and meaningfulness of the latent space transitions\n",
    " 2. Random sampling from the latent space to assess the quality of the generated images and understand what the VAE has learned\n",
    "\n",
    " This comparison will help us understand how a production-grade VAE performs compared to our simpler implementation which was trained on a small dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure autoreload to automatically reload modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import required libraries for dataset handling\n",
    "from icl_dl_cw2_utils.utils.hotdogdataset import DLHotDogDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define image size and batch size parameters\n",
    "# Using larger images since we don't need to train the VAE anymore\n",
    "image_size_ = 112  # Target image size after transforms\n",
    "batch_size = 32    # Number of images per batch\n",
    "\n",
    "# Define image transformations for training data\n",
    "# First resize to 128 then center crop to ensure consistent sizing\n",
    "transform_ = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.CenterCrop(image_size_),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Define image transformations for test data\n",
    "transform_test_ = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((image_size_, image_size_))\n",
    "])\n",
    "\n",
    "# Load training and test datasets with the defined transforms\n",
    "hotdogdata_train_256 = DLHotDogDataset(root=data_path, transform=transform_, split='train', preload=True)\n",
    "hotdogdata_test_256 = DLHotDogDataset(root=data_path, transform=transform_, split='test', preload=True)\n",
    "\n",
    "# Create data loaders for efficient batching\n",
    "# num_workers=8 enables parallel data loading\n",
    "hotdogdata_loader_train_112 = DataLoader(hotdogdata_train_256, batch_size, shuffle=True, num_workers=8)\n",
    "hotdogdata_loader_test_112 = DataLoader(hotdogdata_test_256, batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "# Get a batch of test images to verify dimensions\n",
    "data_iter = iter(hotdogdata_loader_test_112)\n",
    "images, _ = next(data_iter)\n",
    "print(f\"Batch shape: {images.shape} (batch_size, channels, height, width)\")\n",
    "\n",
    "# Calculate and display dataset statistics\n",
    "hotdogdata_mean, hotdogdata_std_dev = hotdogdata_train_256.calculate_mean_and_std()\n",
    "print(f'Dataset statistics:')\n",
    "print(f'Mean pixel value: {hotdogdata_mean:.4f}')\n",
    "print(f'Standard deviation: {hotdogdata_std_dev:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1a - Interpolating in $z$ (1 point)\n",
    "\n",
    "Perform a linear interpolation in the latent space of the autoencoder by choosing any two digits from the test set. What do you observe regarding the transition from on digit to the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CODE FOR PART 2.1a - Linear Interpolation in Latent Space\n",
    "# This code demonstrates how to:\n",
    "# 1. Take two images from the dataset\n",
    "# 2. Encode them into the latent space\n",
    "# 3. Create interpolated points between their latent representations\n",
    "# 4. Decode the interpolated points back to images\n",
    "\n",
    "#######################################################################\n",
    "#                       ** START OF YOUR CODE **\n",
    "#######################################################################\n",
    "\n",
    "# Number of interpolation steps between the two images\n",
    "num_steps = 10\n",
    "\n",
    "# Get a batch of training images\n",
    "data_iter = iter(hotdogdata_loader_train_112)\n",
    "images, _ = ...\n",
    "print(f\"Loaded batch of images with shape: {images.shape}\")\n",
    "\n",
    "# Select first two images and move to device\n",
    "image1 = ...\n",
    "image2 = ...\n",
    "print(f\"Individual image shape: {image1.shape}\")\n",
    "\n",
    "...\n",
    "# Create interpolated points in latent space\n",
    "interpolated_embeddings = ...\n",
    "print(f\"Shape of interpolated embeddings: {interpolated_embeddings.shape}\")         \n",
    "\n",
    "# Decode interpolated points back to images\n",
    "with torch.no_grad():\n",
    "    interpolated_images = ...\n",
    "    print(f\"Shape of decoded interpolated images: {interpolated_images.shape}\")\n",
    "\n",
    "def imshow(img, ax):\n",
    "    \"\"\"Display an image on the given matplotlib axis\"\"\"\n",
    "    img = img.squeeze().permute(1, 2, 0)\n",
    "    ax.imshow(img.cpu().numpy())\n",
    "    ax.axis('off')\n",
    "\n",
    "# Create visualization of original images and interpolations\n",
    "fig, axes = plt.subplots(1, interpolated_images.size(0) + 2, figsize=(20, 5))\n",
    "\n",
    "# Show first original image\n",
    "imshow(image1, axes[0])\n",
    "axes[0].set_title('Original Image 1')\n",
    "\n",
    "# Show interpolated images\n",
    "for i in range(interpolated_images.size(0)):\n",
    "    imshow(interpolated_images[i], axes[i + 1])\n",
    "    axes[i + 1].set_title(f'{i*10}% of Image 2')\n",
    "\n",
    "# Show second original image\n",
    "imshow(image2, axes[-1])\n",
    "axes[-1].set_title('Original Image 2')\n",
    "\n",
    "plt.suptitle('Linear Interpolation Between Two Images in Latent Space', y=0.75)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#######################################################################\n",
    "#                       ** END OF YOUR CODE **\n",
    "####################################################################### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Part 2.1b - Sample visualisation (2 points)\n",
    "visualisation (1 point), written answer (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CODE FOR PART 2.1b - Visualization of VAE Results\n",
    "# This section demonstrates reconstruction and generation capabilities of the trained VAE\n",
    "\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "# Helper function to denormalize images if needed\n",
    "def denorm(x):\n",
    "    return x\n",
    "\n",
    "# 1. Visualize Input Images\n",
    "print('\\n1. Displaying Original Input Images from Test Set')\n",
    "print('-'*70)\n",
    "sample_inputs, _ = next(iter(hotdogdata_loader_test_112))\n",
    "fixed_input = sample_inputs[0:32, :, :, :]\n",
    "input_grid = make_grid(denorm(fixed_input), nrow=8, padding=2, normalize=False,\n",
    "                      scale_each=False, pad_value=0)\n",
    "plt.figure()\n",
    "plt.title('Original Test Set Images')\n",
    "show(input_grid)\n",
    "\n",
    "# 2. Visualize Reconstructed Images\n",
    "print('\\n2. Displaying VAE Reconstructions')\n",
    "print('-'*70)\n",
    "print('The VAE should learn to accurately reconstruct the input images')\n",
    "\n",
    "with torch.no_grad():\n",
    "    #######################################################################\n",
    "    #                       ** START OF YOUR CODE **\n",
    "    # CODE: Implement the reconstruction process:\n",
    "    # 1. Move input images to device\n",
    "    # 2. Encode images to get latent embeddings\n",
    "    # 3. Decode latent embeddings back to images\n",
    "    #######################################################################\n",
    "    ...\n",
    "    #######################################################################\n",
    "    #                       ** END OF YOUR CODE **\n",
    "    #######################################################################\n",
    "    \n",
    "    recon_grid = make_grid(denorm(recon_batch.cpu()), nrow=8, padding=2, normalize=False,\n",
    "                          scale_each=False, pad_value=0)\n",
    "    plt.figure()\n",
    "    plt.title('VAE Reconstructed Images')\n",
    "    show(recon_grid)\n",
    "\n",
    "# 3. Generate New Images from Random Noise\n",
    "print('\\n3. Displaying Generated Images from Random Noise')\n",
    "print('-'*70)\n",
    "print('The VAE should generate plausible new images from random latent vectors')\n",
    "\n",
    "n_samples = 256\n",
    "print(f'Latent embedding shape: {recon_emb.shape}')\n",
    "z = torch.randn_like(recon_emb).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    #######################################################################\n",
    "    #                       ** START OF YOUR CODE **\n",
    "    # CODE: Implement the generation process:\n",
    "    # 1. Sample random noise vectors from normal distribution\n",
    "    # 2. Decode noise vectors to generate new images\n",
    "    #######################################################################\n",
    "    ...\n",
    "    #######################################################################\n",
    "    #                       ** END OF YOUR CODE **\n",
    "    #######################################################################\n",
    "    \n",
    "    samples_grid = make_grid(denorm(samples.cpu()), nrow=16, padding=2, normalize=False,\n",
    "                           scale_each=False, pad_value=0)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title('VAE Generated Images from Random Noise')\n",
    "    show(samples_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question \n",
    "\n",
    "The quality of random samples from the VAE's latent space are poor despite using a well-trained VAE. Please detail why this is the case and why moving to latent diffusion models provides an effective solution. In your answer, discuss the limitations of random sampling and explain how latent diffusion models address these challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Your Answer (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Part 2.2 - Implementing a Latent Diffusion Model (32 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Whilst we provide some details below, for an excellent high-level overview of diffusion models, we highly recommend reading Lilian Weng's blog post:\n",
    " https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\n",
    "\n",
    " However, you will still need to carefully study the mathematical details in the original DDPM paper \n",
    " (https://arxiv.org/abs/2006.11239) to implement the model correctly. We'll first build the pre-computed \n",
    " schedules for the DDPM, then implement the noise predictor model, and finally the DDPM itself. While the DDPM \n",
    " can be used on images directly or on embedding latent codes (as we can generate with our Stable Diffusion VAE), \n",
    " we'll focus on latent diffusion since the latent codes are much smaller and training is significantly faster. \n",
    " This is also practical since we don't have the compute power to use DDPM directly on our 112x112 hot dog images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background \n",
    "\n",
    "\n",
    "\n",
    " Diffusion models learn to gradually denoise data by reversing a fixed forward diffusion process. The process works in two phases:\n",
    "\n",
    "\n",
    "\n",
    " #### Forward Diffusion \n",
    "\n",
    " We define a forward process that gradually adds Gaussian noise to an image x₀ over T timesteps:\n",
    "\n",
    "\n",
    "\n",
    " $q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t\\mathbf{I})$\n",
    "\n",
    "\n",
    "\n",
    " where βₜ is a variance schedule that controls the noise level. This can be written in closed form:\n",
    "\n",
    "\n",
    "\n",
    " $q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1-\\bar{\\alpha}_t)\\mathbf{I})$\n",
    "\n",
    "\n",
    "\n",
    " where $\\bar{\\alpha}_t = \\prod_{s=1}^t (1-\\beta_s)$\n",
    "\n",
    "\n",
    "\n",
    " #### Reverse Diffusion \n",
    "\n",
    " We train a model to reverse this process by predicting the noise at each step:\n",
    "\n",
    "\n",
    "\n",
    " $p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t,t), \\sigma_t^2\\mathbf{I})$\n",
    "\n",
    "\n",
    "\n",
    " The mean is predicted by our noise prediction model:\n",
    "\n",
    "\n",
    "\n",
    " $\\mu_\\theta(x_t,t) = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t,t))$\n",
    "\n",
    "\n",
    "\n",
    " #### Training Objective \n",
    "\n",
    " We train the model to predict the noise ε added during the forward process:\n",
    "\n",
    "\n",
    "\n",
    " $\\mathcal{L} = \\mathbb{E}_{t,x_0,\\epsilon}[\\|\\epsilon - \\epsilon_\\theta(x_t,t)\\|_2^2]$\n",
    "\n",
    "\n",
    "\n",
    " where:\n",
    "\n",
    " - t is sampled uniformly from [1,T]\n",
    "\n",
    " - x₀ is sampled from the training data\n",
    "\n",
    " - ε is sampled from N(0,I)\n",
    "\n",
    " - xₜ is computed using the reparameterization trick\n",
    "\n",
    "\n",
    " #### Latent Diffusion \n",
    "\n",
    " Rather than operating directly on pixels, we'll apply diffusion in the latent space of a pre-trained VAE:\n",
    "\n",
    " 1. Encode images to latent space using VAE encoder\n",
    "\n",
    " 2. Apply diffusion process to latent vectors\n",
    "\n",
    " 3. Decode generated latents to images\n",
    "\n",
    "\n",
    "\n",
    " This significantly reduces computational cost while maintaining generation quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2a - Implement Pre-Computed DDPM Schedules (5 points)\n",
    "\n",
    "The DDPM process requires several pre-computed schedules that control how noise is added and removed during training and sampling. These schedules are carefully designed to:\n",
    "\n",
    "1. Gradually corrupt the data with noise in a controlled way during the forward process\n",
    "2. Allow the model to learn to denoise effectively during training  \n",
    "3. Enable high-quality sample generation during the reverse process\n",
    "\n",
    "The key schedules are:\n",
    "\n",
    "Linear noise schedule $\\beta_t$ that increases from $\\beta_1$ to $\\beta_2$:\n",
    "\n",
    "$\\beta_t = \\beta_1 + (\\beta_2 - \\beta_1)\\frac{t}{T}$\n",
    "\n",
    "This schedule starts with small noise ($\\beta_1$) and gradually increases to larger noise ($\\beta_2$), allowing for:\n",
    "- Initial steps that preserve more image structure\n",
    "- Later steps that add more randomness\n",
    "\n",
    "Alpha values derived from betas:\n",
    "$\\alpha_t = 1 - \\beta_t$\n",
    "\n",
    "Cumulative product of alphas:\n",
    "$\\bar{\\alpha}_t = \\prod_{i=1}^t (1 - \\beta_i)$\n",
    "\n",
    "Other important quantities and their purposes:\n",
    "- $\\frac{1}{\\sqrt{\\alpha_t}}$: Used in mean calculation to properly scale the denoising prediction\n",
    "- $\\sqrt{\\beta_t}$: Controls the standard deviation of noise added at each step  \n",
    "- $\\sqrt{\\bar{\\alpha}_t}$: Coefficient for $x_0$ term - determines how much original signal remains\n",
    "- $\\sqrt{1-\\bar{\\alpha}_t}$: Coefficient for noise term - determines how much noise has accumulated\n",
    "\n",
    "These values enable us to:\n",
    "1. Sample from the forward process (adding noise):\n",
    "$q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1-\\bar{\\alpha}_t)\\mathbf{I})$\n",
    "\n",
    "2. Sample from the reverse process (removing noise):\n",
    "$p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t,t), \\sigma_t^2\\mathbf{I})$\n",
    "\n",
    "The careful balancing of these schedules is crucial for stable training and high-quality generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CODE FOR PART 2.2a IN THIS CELL\n",
    "\n",
    "def ddpm_schedules(beta1: float, beta2: float, T: int) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns pre-computed schedules for DDPM sampling and training process.\n",
    "    \n",
    "    Args:\n",
    "        beta1: Starting value of noise schedule (must be between 0 and 1)\n",
    "        beta2: Ending value of noise schedule (must be between beta1 and 1)\n",
    "        T: Number of timesteps in the diffusion process\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing the following tensors of shape (T+1,):\n",
    "            alpha_t: The alpha schedule values\n",
    "            oneover_sqrta: 1/sqrt(alpha_t) for scaling in diffusion process\n",
    "            sqrt_beta_t: sqrt(beta_t) for noise scaling\n",
    "            alphabar_t: The cumulative product of (1-beta) \n",
    "            sqrtab: sqrt(alphabar_t) for x0 coefficient\n",
    "            sqrtmab: sqrt(1-alphabar_t) for epsilon coefficient\n",
    "            mab_over_sqrtmab: (1-alpha_t)/sqrt(1-alphabar_t) for posterior variance\n",
    "    \"\"\"\n",
    "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "    #######################################################################\n",
    "    #                       ** START OF YOUR CODE **\n",
    "    #######################################################################\n",
    "    ...\n",
    "    #######################################################################\n",
    "    #                       ** END OF YOUR CODE **\n",
    "    ####################################################################### \n",
    "\n",
    "    return {\n",
    "        \"alpha_t\": alpha_t,  # \\alpha_t\n",
    "        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n",
    "        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n",
    "        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n",
    "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
    "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
    "        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q2.2a DDPM Schedule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ddpm_schedules(beta1: float, beta2: float, T: int):\n",
    "    \"\"\"\n",
    "    Plots the DDPM schedules to visualize how they change over timesteps.\n",
    "    \n",
    "    Args:\n",
    "        beta1: Starting value of noise schedule\n",
    "        beta2: Ending value of noise schedule  \n",
    "        T: Number of timesteps\n",
    "    \"\"\"\n",
    "    schedules = ddpm_schedules(beta1, beta2, T)\n",
    "    \n",
    "    explanations = {\n",
    "        \"alpha_t\": \"Complement of noise level (1-β)\\nControls signal preservation\",\n",
    "        \"oneover_sqrta\": \"Scaling factor 1/√α\\nUsed in denoising prediction\",\n",
    "        \"sqrt_beta_t\": \"√β: Standard deviation\\nof noise added per step\",\n",
    "        \"alphabar_t\": \"ᾱ: Cumulative product of (1-β)\\nTotal signal remaining\",\n",
    "        \"sqrtab\": \"√ᾱ: Coefficient for x₀\\nScales original image\",\n",
    "        \"sqrtmab\": \"√(1-ᾱ): Coefficient for ε\\nScales noise component\",\n",
    "        \"mab_over_sqrtmab\": \"(1-α)/√(1-ᾱ)\\nPosterior variance scaling\"\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot each schedule on separate subplots\n",
    "    for idx, (name, values) in enumerate(schedules.items(), 1):\n",
    "        plt.subplot(3, 3, idx)\n",
    "        plt.plot(values.numpy())\n",
    "        plt.title(f\"{name}\\n{explanations[name]}\", fontsize=10)\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('Value')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_ddpm_schedules(0.0001, 0.02, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Part 2.2b - Noise Predictor Model (10 points)\n",
    "\n",
    " The noise predictor model is a crucial component of the diffusion process that learns to predict and remove noise from images. It takes a noisy image and timestep as input and predicts the noise that was added, enabling the gradual denoising process.\n",
    "\n",
    " Define a $R_n \\rightarrow R_n$ noise predictor model with the following options:\n",
    "\n",
    " 1. Simple CNN Approach (6 points):\n",
    "    * A basic multi-layer CNN that maps noisy images to predicted noise\n",
    "    * Simpler architecture but can still be effective for denoising\n",
    "    * Example structure: Multiple conv layers with increasing then decreasing channels\n",
    "\n",
    " 2. U-Net Approach (+2 points):\n",
    "    * More sophisticated architecture commonly used in diffusion models\n",
    "    * Includes skip connections to preserve spatial information\n",
    "    * Better at capturing both local and global image features\n",
    "\n",
    " 3. Positional Encoding (+2 points):\n",
    "    * Add timestep embeddings to help the model understand the noise level\n",
    "    * Can be implemented as sinusoidal embeddings or learned embeddings\n",
    "    * Critical for conditioning the model on different timesteps\n",
    "\n",
    " Requirements:\n",
    " * The model MUST NOT exceed 20M parameters\n",
    " * Implementing both architectures (CNN and UNet) will not yield additional points\n",
    " * Choose one approach and implement it well\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CODE FOR PART 2.2b simple CNN IN THIS CELL (or use cell below for UNet with or without positional encoding)\n",
    "\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import torch.nn as nn\n",
    "\n",
    "block = lambda ic, oc: nn.Sequential(\n",
    "    nn.Conv2d(ic, oc, 7, padding=3),\n",
    "    nn.BatchNorm2d(oc),\n",
    "    nn.LeakyReLU(),\n",
    ")\n",
    "\n",
    "class SimpleEpsModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Basically, any universal R^n -> R^n model should work for denoising, so try some very simple CNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channel: int) -> None:\n",
    "        super(SimpleEpsModel, self).__init__()\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        ...\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def forward(self, x, t) -> torch.Tensor:\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        # In the paper, they used Tr-like positional embeddings but t is not absolutly necessary, depending on your model.\n",
    "        ...\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "    \n",
    "eps_model = SimpleEpsModel(4)\n",
    "print(\"Num params: \", sum(p.numel() for p in eps_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic building block for the U-Net architecture that processes both spatial and temporal information.\n",
    "    \n",
    "    Args:\n",
    "        in_ch (int): Number of input channels\n",
    "        out_ch (int): Number of output channels \n",
    "        time_emb_dim (int): Dimension of time embedding\n",
    "        up (bool): If True, uses transposed convolution for upsampling. If False, uses regular convolution for downsampling\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        ...\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Forward pass of the block.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input feature maps\n",
    "            t (torch.Tensor): Time embeddings\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Transformed feature maps\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        \n",
    "        # First Conv + Norm + Activation\n",
    "        h = ...\n",
    "        \n",
    "        # Process time embeddings\n",
    "        time_emb = ...\n",
    "        # Add two dummy spatial dimensions to match feature map\n",
    "        time_emb = ...\n",
    "        # Add time information to features\n",
    "        h = ...\n",
    "        \n",
    "        # Second Conv + Norm + Activation\n",
    "        h = ...\n",
    "        # Final transformation (up/downsampling)\n",
    "        ...\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates sinusoidal positional embeddings for time steps.\n",
    "    Uses alternating sine and cosine functions at different frequencies.\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Dimension of the embeddings\n",
    "    \"\"\"\n",
    "    #######################################################################\n",
    "    #                       ** START OF YOUR CODE **\n",
    "    #######################################################################\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        ...\n",
    "    \n",
    "    #######################################################################\n",
    "    #                       ** END OF YOUR CODE **\n",
    "    #######################################################################\n",
    "\n",
    "    def forward(self, time):\n",
    "        \"\"\"\n",
    "        Compute positional embeddings for given timesteps.\n",
    "        \n",
    "        Args:\n",
    "            time (torch.Tensor): Tensor of timesteps\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Position embeddings of shape (batch_size, dim)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        ...\n",
    "        embeddings = ...\n",
    "        \n",
    "        return embeddings\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "class SimpleUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified variant of the Unet architecture for diffusion models.\n",
    "    Includes time conditioning and skip connections.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input image channels\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=4):\n",
    "        super().__init__()\n",
    "        image_channels = in_channels\n",
    "        down_channels = (128, 256, 512)  # Limited the downsampling stages\n",
    "        up_channels = (512, 256, 128)\n",
    "        out_dim = in_channels\n",
    "        time_emb_dim = 256\n",
    "\n",
    "        # Time embedding layers\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "        # Initial projection of image\n",
    "        self.conv0 = ...\n",
    "\n",
    "        # Downsampling path\n",
    "        self.downs = ...\n",
    "        # Bottleneck block\n",
    "        self.bottleneck = ...\n",
    "\n",
    "        # Upsampling path\n",
    "        self.ups = ...\n",
    "\n",
    "        # Final projection to output channels\n",
    "        self.output = ...\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        \"\"\"\n",
    "        Forward pass of the U-Net.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width)\n",
    "            timestep (torch.Tensor): Current timestep for conditioning\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of same shape as input\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        # Get time embeddings\n",
    "        t = ...\n",
    "        \n",
    "        # Initial convolution\n",
    "        x = ...\n",
    "        \n",
    "        # Store intermediate outputs for skip connections\n",
    "        residual_inputs = []\n",
    "        # Downsampling path\n",
    "        ...\n",
    "        # Bottleneck\n",
    "        x = ...\n",
    "        \n",
    "        # Upsampling path with skip connections\n",
    "        ...\n",
    "        ...\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "# Test the model, your latent input shape will only be [4,14,14], which means that kernel sizes can quickly get larger than input sizes\n",
    "input_tensor = torch.randn(1, 4, 14, 14)\n",
    "timestep = torch.tensor([1.0])\n",
    "UNetModel = SimpleUnet()\n",
    "output = UNetModel(input_tensor, timestep)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Num params: \", sum(p.numel() for p in UNetModel.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Part 2.2c - Defition of the DDPM (10 points)\n",
    "\n",
    "Now we define the DDPM. Remember that the noise predictor model's task is only to predict the amount of noise between time step $T$ and $T+n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CODE FOR PART 2.2c IN THIS CELL \n",
    "\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    \"\"\"\n",
    "    Denoising Diffusion Probabilistic Model (DDPM) implementation.\n",
    "    \n",
    "    This class implements the DDPM as described in \"Denoising Diffusion Probabilistic Models\" \n",
    "    (Ho et al., 2020). The model learns to reverse a gradual noising process.\n",
    "    \n",
    "    Args:\n",
    "        eps_model (nn.Module): The neural network that predicts noise at each timestep\n",
    "        betas (Tuple[float, float]): Beta schedule parameters (β_start, β_end)\n",
    "        n_T (int): Number of timesteps in the diffusion process\n",
    "        criterion (nn.Module): Loss function for training, defaults to MSELoss\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        eps_model: nn.Module,\n",
    "        betas: Tuple[float, float],\n",
    "        n_T: int,\n",
    "        criterion: nn.Module = nn.MSELoss()\n",
    "    ) -> None:\n",
    "        super(DDPM, self).__init__()\n",
    "        self.eps_model = eps_model\n",
    "\n",
    "        # register_buffer allows us to freely access these tensors by name. It helps device placement.\n",
    "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
    "            self.register_buffer(k, v)\n",
    "\n",
    "        self.n_T = n_T\n",
    "        self.criterion = criterion\n",
    "\n",
    "        # Initialize with dataset statistics for potentially better starting point\n",
    "        mean =  0.04640255868434906\n",
    "        std_dev = 0.8382343649864197\n",
    "        self.normal_dist = dist.Normal(mean, std_dev)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs forward diffusion and predicts the noise added at timestep t.\n",
    "        \n",
    "        This implements Algorithm 1 from the DDPM paper:\n",
    "        1. Sample a random timestep t\n",
    "        2. Sample random noise ε\n",
    "        3. Create noised input x_t using the noise schedule\n",
    "        4. Predict the noise using the model\n",
    "        5. Return the loss between predicted and actual noise\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input images/data\n",
    "            t (torch.Tensor, optional): Specific timestep. If None, randomly sampled.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "            - Loss between predicted and actual noise\n",
    "            - The sampled noise (eps)\n",
    "            - The noised input at timestep t (x_t)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "        # Step 1: Sample timestep t if not provided\n",
    "        ...\n",
    "        # Step 2: Sample noise from normal distribution\n",
    "        ...\n",
    "        # Step 3: Create noised input x_t using the forward process\n",
    "        # x_t = √(αbar_t) * x_0 + √(1-αbar_t) * ε\n",
    "        ...\n",
    "        # Step 4 & 5: Predict noise using eps_model and return loss\n",
    "        # Note: timesteps are normalized to [0,1] range for the model\n",
    "        ...\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "    \n",
    "    def sample(self, n_sample: int, size, device, t = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Samples new images using the trained diffusion model.\n",
    "        \n",
    "        Implements Algorithm 2 from the DDPM paper - the reverse diffusion process.\n",
    "        Starting from pure noise, gradually denoise to generate new samples.\n",
    "        \n",
    "        Args:\n",
    "            n_sample (int): Number of samples to generate\n",
    "            size (tuple): Size of each sample\n",
    "            device: Device to generate samples on\n",
    "            t (int): Starting timestep (default=0)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Generated samples\n",
    "        \"\"\"\n",
    "        # Start from pure noise\n",
    "        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1)\n",
    "        \n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        # Gradually denoise the samples\n",
    "        for i in range(self.n_T, t, -1):\n",
    "            ...\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return x_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Part 2.2d - Forward Diffusion Visualisation (7 points)\n",
    "\n",
    "Let's try if the forward process works. Provide 10 samples of a randomly chosen hot-dog between time steps T = 0 (min) to T = 1000 (max), encode it, gradually noise the latent code with your forward model and decode it again. \n",
    "Hint: no training is required for this step.\n",
    "\n",
    "<img src=\"https://www.doc.ic.ac.uk/~bkainz/teaching/DL/example_noise.png\" alt=\"Hot-dog Generator Model\" width=\"700\">\n",
    "\n",
    "*Figure 4: here is an example how this could look like.* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CODE FOR PART 2.2d IN THIS CELL \n",
    "\n",
    "import torch.distributions as dist\n",
    "\n",
    "# Helper function to display tensor images\n",
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "    # Take first image of batch\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0, :, :, :] \n",
    "    plt.imshow(reverse_transforms(image))\n",
    "\n",
    "# Set up DDPM model parameters\n",
    "print(\"Setting up DDPM model with 1000 timesteps...\")\n",
    "T = 1000\n",
    "\n",
    "#######################################################################\n",
    "#                       ** START OF YOUR CODE **\n",
    "#######################################################################\n",
    "\n",
    "ddpm = ...\n",
    "\n",
    "# Get a sample image and encode it to latent space\n",
    "print(\"\\nStep 1: Loading and encoding a sample hot dog image\")\n",
    "image = next(iter(hotdogdata_loader_test_112))[0]\n",
    "print(f\"Original image shape: {image.shape}\")\n",
    "\n",
    "image = image.to(device)\n",
    "\n",
    "emb = ...\n",
    "\n",
    "latent_shape = emb.shape\n",
    "print(f\"Encoded latent shape: {emb.shape}\")\n",
    "\n",
    "# Visualize forward diffusion process\n",
    "print(\"\\nStep 2: Visualizing forward diffusion process over time\")\n",
    "print(\"Creating plot with 10 timesteps from t=0 to t=T...\")\n",
    "plt.figure(figsize=(20,2))  \n",
    "plt.axis('off')\n",
    "num_images = 10\n",
    "stepsize = int(T/num_images)\n",
    "\n",
    "# Apply noise gradually and show results\n",
    "print(\"\\nStep 3: Applying noise gradually and decoding at each step...\")\n",
    "for idx in range(0, T, stepsize):\n",
    "    ...\n",
    "    plt.subplot(1, num_images+1, int(idx/stepsize) + 1)\n",
    "    plt.axis('off')\n",
    "    ...\n",
    "    img = ...\n",
    "    show_tensor_image(img)\n",
    "plt.tight_layout()  # Adjust spacing between subplots\n",
    "print(\"\\nVisualization complete - observe how the image becomes increasingly noisy over time\")\n",
    "\n",
    "#######################################################################\n",
    "#                       ** END OF YOUR CODE **\n",
    "#######################################################################  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<h3> Hotdog or not? </h3>\n",
    "\n",
    "We provide a ready made food classifier for evaluation. The hot dog category is 55 in the classifier's output.\n",
    "This classifier is a ResNet model trained on a large food image dataset with 101 categories.\n",
    "It outputs a probability distribution over all food categories, where index 55 corresponds to hot dogs.\n",
    "We will use this classifier to evaluate how realistic our generated hot dog images are by checking\n",
    "the confidence score (probability) it assigns to the hot dog category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icl_dl_cw2_utils.utils.food_resnet import HotDogClassifier\n",
    "hotdogclassifier = HotDogClassifier(checkpoint_path=content_path/'hotdogdetect_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.3 - Training the Latent Diffusion Model (15 points)\n",
    "\n",
    "In this section, you will train a Latent Diffusion Model (LDM) to generate hot dog images. Here are the key steps and considerations:\n",
    "\n",
    "### Training Process\n",
    "1. First, encode your training hot dog images into the latent space using the pre-trained VAE encoder\n",
    "2. Train the DDPM model on these latent representations\n",
    "3. The training process typically takes 30-60 minutes to see initial meaningful results\n",
    "\n",
    "### What to Expect During Training\n",
    "* Initial epochs (first ~20-30): The generated images will look like pure noise\n",
    "* Middle epochs: You'll start seeing food-like textures and patterns emerge\n",
    "* Later epochs: The images should begin (very crudely) resembling hot dogs\n",
    "* Note: More training time doesn't always mean better results - watch for overfitting\n",
    "\n",
    "### Success Criteria\n",
    "* Your goal is to generate images with hotdog likeness exceeding or roughly matching the examples provided in figure 6 \n",
    "* Save both your model weights and the successful generated image(s)\n",
    "\n",
    "### Model Architecture Options\n",
    "You have two main choices for the noise prediction network:\n",
    "1. SimpleEpsModel (~16M parameters)\n",
    "   * Faster initial convergence (~20 epochs)\n",
    "   * May plateau in quality relatively early\n",
    "   * Example results shown in Figure 5\n",
    "   \n",
    "2. SimpleUnet with positional encoding (~16M parameters)\n",
    "   * Takes longer to show initial results (~30 epochs)\n",
    "   * Often achieves better final quality\n",
    "   * Example results shown in Figure 6\n",
    "\n",
    "### Example Results\n",
    "<img src=\"https://www.doc.ic.ac.uk/~bkainz/teaching/DL/53hotdog.png\" alt=\"Hot-dog Generator Model\" width=\"200\">\n",
    "<img src=\"https://www.doc.ic.ac.uk/~bkainz/teaching/DL/80hotdog.png\" alt=\"Hot-dog Generator Model\" width=\"200\">\n",
    "\n",
    "*Figure 5: Examples of generated hot dogs using SimpleEpsModel. Left: 53% confidence score, Right: 80% confidence score. Generated after 1 hour of training with 16M parameters.* \n",
    "\n",
    "<img src=\"https://www.doc.ic.ac.uk/~bkainz/teaching/DL/unethotdog1.png\" alt=\"Hot-dog Generator Model\" width=\"200\">\n",
    "<img src=\"https://www.doc.ic.ac.uk/~bkainz/teaching/DL/unethotdog2.png\" alt=\"Hot-dog Generator Model\" width=\"200\">\n",
    "\n",
    "*Figure 6: Examples of generated hot dogs using UNet with positional encoding. These results demonstrate better texture and structural coherence compared to SimpleEpsModel. Generated after 1 hour of training with 16M parameters.*\n",
    "\n",
    "### Tips for Success\n",
    "* Monitor your training loss - it should generally decrease over time\n",
    "* Save checkpoints periodically in case you need to revert to a better model\n",
    "* Try both model architectures to see which works better for your setup\n",
    "* Consider using a smaller batch size if you run into memory issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CODE FOR PART 2.2e IN THIS CELL \n",
    "\n",
    "#######################################################################\n",
    "#                       ** START OF YOUR CODE **\n",
    "#######################################################################\n",
    "\n",
    "# ===== 1. Setup Model Architecture and Parameters =====\n",
    "# Initialize shapes by processing a sample image through VAE\n",
    "...\n",
    "emb_shape = ...\n",
    "img_shape = ...\n",
    "latent_shape = ...\n",
    "print(\"Input image shape (batch_size, channels, height, width):\", img_shape)\n",
    "print(\"VAE encoded latent shape:\", emb_shape)\n",
    "print(\"Final latent shape for model input:\", latent_shape)\n",
    "\n",
    "# ===== 2. Model Selection =====\n",
    "# Choose between SimpleEpsModel (faster convergence) or SimpleUnet (better quality)\n",
    "rn_rn_model = None\n",
    "model_choice = 1  # 0 for SimpleEpsModel, 1 for SimpleUnet\n",
    "\n",
    "if model_choice == 0:\n",
    "    # SimpleEpsModel: Faster initial convergence (~20 epochs)\n",
    "    eps_model = ...\n",
    "    rn_rn_model = eps_model.to(device)\n",
    "    print(\"SimpleEpsModel total trainable parameters:\", sum(p.numel() for p in eps_model.parameters()))\n",
    "    model_path = content_path/'CW_LDM/ldm_hotdogs_dummyeps.pth'\n",
    "elif model_choice == 1:\n",
    "    # SimpleUnet: Better final quality but slower convergence (~30 epochs)\n",
    "    simple_unet = ...\n",
    "    rn_rn_model = simple_unet.to(device)\n",
    "    print(\"SimpleUnet total trainable parameters:\", sum(p.numel() for p in simple_unet.parameters()))\n",
    "    model_path = content_path/'CW_LDM/ldm_hotdogs_simple_unet_cls.pth'\n",
    "\n",
    "# ===== 3. Initialize DDPM and Optimizer =====\n",
    "print(\"Initializing DDPM model and optimizer...\")\n",
    "lr = 2e-4 * batch_size  # Learning rate scaled by batch size\n",
    "ddpm = ...\n",
    "optim = ...\n",
    "\n",
    "#######################################################################\n",
    "#                       ** END OF YOUR CODE **\n",
    "#######################################################################  \n",
    "\n",
    "# ===== 4. Checkpoint Loading (Optional) =====\n",
    "lastepoch = 0\n",
    "load_checkpoint = True\n",
    "if os.path.exists(model_path) and load_checkpoint:\n",
    "    print(\"Loading existing model checkpoint from:\", model_path)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    ddpm.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lastepoch = checkpoint['epoch']\n",
    "\n",
    "ddpm.to(device)\n",
    "\n",
    "# ===== 5. Training Loop =====\n",
    "# Initialize tracking variables\n",
    "example = None\n",
    "input_sample = None\n",
    "best_hd_prob = 0\n",
    "best_hd_image = None\n",
    "\n",
    "# Initialize widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "n_epoch = 5  # Train for at least a few hundred epochs (several hours)\n",
    "\n",
    "# Progress bar to track training progress across epochs\n",
    "progress_bar = widgets.FloatProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=n_epoch,\n",
    "    description=f'Training (0/{n_epoch} epochs):',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "loss_text = widgets.HTML(value='Current loss: -')\n",
    "image_output = widgets.Output()\n",
    "display(widgets.VBox([progress_bar, loss_text, image_output]))\n",
    "\n",
    "# Main training loop\n",
    "for i in range(lastepoch, n_epoch):\n",
    "    # Training phase\n",
    "    ddpm.train()\n",
    "    batch_pbar = tqdm(hotdogdata_loader_train_112, leave=False)\n",
    "    loss_ema = None\n",
    "    loss_comb_ema = None\n",
    "    \n",
    "    # Process each batch\n",
    "    for x, _ in batch_pbar:\n",
    "        optim.zero_grad()\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        ...\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        batch_pbar.set_description(f\"Training loss (EMA): {loss_ema:.4f}\")\n",
    "        optim.step()\n",
    "    # Update progress widgets\n",
    "    progress_bar.value = i\n",
    "    progress_bar.description = f'Training ({i+1}/{n_epoch} epochs):'\n",
    "    loss_text.value = f'Current loss: {loss_ema:.4f}'\n",
    "\n",
    "    # ===== 6. Evaluation and Sample Generation =====\n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        # Generate samples\n",
    "        xh = ddpm.sample(16, (latent_shape[1], latent_shape[2], latent_shape[3]), device)\n",
    "        xh = sd_vae.decode(xh)\n",
    "        \n",
    "        # Save generated and input samples\n",
    "        grid = make_grid(xh, nrow=4)\n",
    "        save_image(grid, content_path/f'CW_LDM/ldm_sample_{i}.png')\n",
    "        grid1 = make_grid(x, nrow=4)\n",
    "        save_image(grid1, content_path/f'CW_LDM/input_sample_{i}.png')\n",
    "        \n",
    "        # Evaluate with hotdog classifier\n",
    "        predictions, probabilities, top5_accuracy = hotdogclassifier.predict(xh.detach())\n",
    "        \n",
    "        # Track best results\n",
    "        example = xh\n",
    "        input_sample = x\n",
    "        column_55_values = probabilities[:, 55]\n",
    "        best_hd_prob_idx = torch.argmax(column_55_values)\n",
    "        best_hd_prob_current = torch.max(column_55_values)\n",
    "        \n",
    "        # Update and display best hotdog image\n",
    "        if best_hd_prob_current > best_hd_prob:\n",
    "            best_hd_prob = best_hd_prob_current\n",
    "            best_hd_image = xh[best_hd_prob_idx,:,:,:]\n",
    "            \n",
    "            with image_output:\n",
    "                clear_output(wait=True)\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                show(best_hd_image)\n",
    "                plt.title(f'Best generated hot dog (confidence: {best_hd_prob:.3f})')\n",
    "                display(plt.gcf())\n",
    "                plt.close()\n",
    "        \n",
    "        # Save checkpoint\n",
    "        state = {\n",
    "            'epoch': i,\n",
    "            'model_state_dict': ddpm.state_dict(),\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'loss': loss.item(),\n",
    "        }\n",
    "        torch.save(state, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    " Congratulations on completing this exercise! We really hope you enjoyed exploring diffusion models and seeing how they can generate interesting images.\n",
    "\n",
    " To reach state-of-the-art performance, several improvements could be made:\n",
    " 1. Data: Using a much larger, high-quality dataset with more diverse food images\n",
    " 2. Compute: Training for longer with more powerful GPUs to handle larger models\n",
    " 3. Architecture: Using more sophisticated architectures like U-Net with attention\n",
    " 4. Resolution: Adding super-resolution models to generate higher-res outputs\n",
    " 5. Guidance: Implementing classifier-free guidance or classifier guidance\n",
    " 6. Conditioning: Adding text conditioning for more controlled generation\n",
    " 7. Training stability: Using techniques like gradient clipping and learning rate scheduling\n",
    "\n",
    " The current implementation provides a great foundation for understanding diffusion models.\n",
    " Building on this, you could explore these advanced techniques to push the boundaries further!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a grid of images using the DDPM model\n",
    "print('Generating images...')\n",
    "print('-'*50)\n",
    "\n",
    "ddpm.eval() \n",
    "with torch.no_grad():\n",
    "    # Generate a batch of images in latent space\n",
    "    n_samples = 36\n",
    "    \n",
    "    # Generate samples in latent space using the DDPM's sample method\n",
    "    # Use the correct latent dimensions from earlier (latent_shape)\n",
    "    x_latent = ddpm.sample(n_samples, \n",
    "                          (latent_shape[1], latent_shape[2], latent_shape[3]), \n",
    "                          device)\n",
    "    \n",
    "    # Decode the latent samples back to image space using the VAE decoder\n",
    "    x = sd_vae.decode(x_latent)\n",
    "    \n",
    "    # Move to CPU and create grid\n",
    "    x = x.cpu()\n",
    "    grid = make_grid(x, nrow=6, padding=2, normalize=True,\n",
    "                    value_range=(-1, 1), scale_each=False, pad_value=0)\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(12,12))\n",
    "    show(grid)\n",
    "    plt.title('Generated Images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "Q1.1a Implement VAE": {
     "name": "Q1.1a Implement VAE",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> sum((p.numel() for p in model.parameters() if p.requires_grad)) < 1000000\nTrue",
         "failure_message": "Model Size Test Failed",
         "hidden": false,
         "locked": false,
         "points": 1,
         "success_message": "Model Size Test Passed"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Q2.2a DDPM Schedule": {
     "name": "Q2.2a DDPM Schedule",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> T = 100\n>>> schedules = ddpm_schedules(0.0001, 0.2, T)\n>>> passed = True\n>>> for (_, values) in schedules.items():\n...     if values.shape[0] != T + 1:\n...         passed = False\n>>> passed\nTrue",
         "failure_message": "Schedule Shape Test Failed",
         "hidden": false,
         "locked": false,
         "points": 1,
         "success_message": "Schedule Shape Test Passed"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
